{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.applications import DenseNet121, VGG16, VGG19, InceptionV3, ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Activation, concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDirectoryIterator import MyDirectoryIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for training, tuning and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COX_GROUP_SIZE = 32  # tile group size for compute cox loss\n",
    "MODEL_BATCH_SIZE = 2  # batch size for model training\n",
    "\n",
    "EPOCHS = 100\n",
    "IMG_ORI_SIZE = 1196  # roi img size\n",
    "IMG_SIZE = 224  # tile img size for network input\n",
    "CHANNEL = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h5_file = '/home/wf/model/progosis.h5'\n",
    "\n",
    "train_dir = '/media/wf/Data/Progosis/train/'\n",
    "tune_dir = '/media/wf/Data/Progosis/tune/'\n",
    "test_dir = '/media/wf/Data/Progosis/test/'\n",
    "\n",
    "metadata_path = '../data/metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "# mode = \"load\"\n",
    "model_h5_file = '/home/wf/model/progosis.h5'\n",
    "\n",
    "train_dir = '/media/wf/移动盘3/Data/20201119/20210325/tcga/finaltcgatrain/'\n",
    "tune_dir = '/media/wf/移动盘3/Data/20201119/20210325/tcga/finaltcgavalid/'\n",
    "test_dir = '/media/wf/移动盘3/Data/20201119/20210325/tcga/finaltcgatest/'\n",
    "metadata_path = '/home/wf/Data/SCNN/metadata/final20201121.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator and data augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pid_by_filename(filename):\n",
    "    return filename.split('_')[1][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(img_dir):\n",
    "    filenames = glob(img_dir+'/*')\n",
    "    pids = []\n",
    "    for filename in filenames:\n",
    "        pid = get_pid_by_filename(os.path.basename(filename))  \n",
    "        pids.append(pid)\n",
    "    return np.array(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read surival and censor data for pids\n",
    "censor_map = {\"Dead\": 0, \"Alive\": 1}\n",
    "def load_metadata(path, pids):\n",
    "    metadata = pd.read_csv(path)\n",
    "    metadata_dict = dict()\n",
    "    for _, row in metadata.iterrows():\n",
    "        metadata_dict[row['TCGA ID']] = row\n",
    "    \n",
    "    survival = np.empty((len(pids), 1),  dtype='int32')\n",
    "    censored = np.empty((len(pids), 1), dtype='int32')\n",
    "    sur_map = {}\n",
    "    cen_map = {}\n",
    "    \n",
    "    error = False\n",
    "    for i, pid in enumerate(pids):\n",
    "        try:\n",
    "            sur_map[pid] = survival[i] = metadata_dict[pid]['survival']\n",
    "            cen_map[pid] = censored[i] = censor_map[metadata_dict[pid]['censored']]\n",
    "        except KeyError:\n",
    "            error = True\n",
    "            print(\"keyerror for\", pid)\n",
    "    if error:\n",
    "        raise Exception(\"load metadata error\")\n",
    "        \n",
    "    return survival, censored, sur_map, cen_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = load_imgs(train_dir)\n",
    "v_pids = load_imgs(tune_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, _, sur_map, cen_map = load_metadata(metadata_path, pids)\n",
    "_, _, v_sur_map, v_cen_map = load_metadata(metadata_path, v_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the training set by survival time\n",
    "# unique\n",
    "tr_pids = np.unique(pids)\n",
    "tr_surs = np.array([sur_map[pid] for pid in tr_pids])\n",
    "tr_cens = np.array([cen_map[pid] for pid in tr_pids])\n",
    "# sort\n",
    "indices = np.argsort(tr_surs)\n",
    "\n",
    "tr_pids = tr_pids[indices]\n",
    "tr_surs = tr_surs[indices]\n",
    "tr_cens = tr_cens[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning set\n",
    "v_pids = np.unique(v_pids) \n",
    "v_surs = np.array([v_sur_map[pid] for pid in v_pids])\n",
    "v_cens = np.array([v_cen_map[pid] for pid in v_pids])\n",
    "\n",
    "indices = np.argsort(v_surs)\n",
    "\n",
    "v_pids = v_pids[indices]\n",
    "v_surs = v_surs[indices]\n",
    "v_cens = v_cens[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_start = random.randint(0, 1196-IMG_SIZE)\n",
    "j_start = random.randint(0, 1196-IMG_SIZE)\n",
    "\n",
    "\"\"\"\n",
    "    build data generator\n",
    "    group_size：tile count for model compute loss once\n",
    "    batch：batch for deep learninig\n",
    "\"\"\"\n",
    "def data_generator(datagen, img_dir, sur_map, cen_map, group_size, batch=1):\n",
    "    \n",
    "    train_iter = MyDirectoryIterator(img_dir, datagen, target_size=(IMG_ORI_SIZE, IMG_ORI_SIZE), batch_size=group_size, shuffle=True)\n",
    "    # buffer acts as a cache, continuously stuffing data into the buffer and yielding when the batch number arrives\n",
    "    img_buffer = [[] for i in range(group_size)]\n",
    "    pid_buffer = []\n",
    "    sur_buffer = []\n",
    "    cen_buffer = []\n",
    "    \n",
    "    for img_batch, pid_batch in train_iter:\n",
    "        if len(pid_batch) < group_size:\n",
    "            continue\n",
    "        pid_batch = np.array([get_pid_by_filename(pid) for pid in pid_batch])\n",
    "        sur_batch = np.array([sur_map[pid] for pid in pid_batch])\n",
    "        cen_batch = np.array([cen_map[pid] for pid in pid_batch])\n",
    "        \n",
    "        # sort\n",
    "        indices = np.argsort(sur_batch)\n",
    "        pid_batch = pid_batch[indices]\n",
    "        img_batch = img_batch[indices]\n",
    "        sur_batch = sur_batch[indices]\n",
    "        cen_batch = cen_batch[indices]\n",
    "        \n",
    "        for i in range(group_size):\n",
    "            img_buffer[i].append(img_batch[i][i_start:(i_start+IMG_SIZE), j_start:(j_start+IMG_SIZE), :])\n",
    "\n",
    "        pid_buffer.append(pid_batch)\n",
    "        sur_buffer.append(sur_batch)\n",
    "        cen_buffer.append(cen_batch)\n",
    "\n",
    "        if len(sur_buffer) == batch:\n",
    "            img_buffer = [np.array(imgs, dtype='float32') for imgs in img_buffer]\n",
    "            yield [*img_buffer, np.array(cen_buffer, dtype='float32')], None\n",
    "            img_buffer = [[] for i in range(group_size)]\n",
    "            pid_buffer = []\n",
    "            sur_buffer = []\n",
    "            cen_buffer = []\n",
    "\n",
    "gen = data_generator(datagen_train, train_dir, sur_map, cen_map, COX_GROUP_SIZE, MODEL_BATCH_SIZE)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build risk model with VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer = base_model.get_layer('block5_pool')\n",
    "\n",
    "conv_model = Model(inputs=base_model.input,\n",
    "                   outputs=transfer_layer.output)\n",
    "\n",
    "new_model = Sequential()\n",
    "\n",
    "new_model.add(conv_model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dropout(0.3))\n",
    "new_model.add(Dense(1))\n",
    "\n",
    "risk_model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build prognostic model with COX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Need not sort by survival, besause it've been done in data generator, which make the training more stable.\n",
    "def compute_loss(risks, censored):   \n",
    "    observed = 1 - censored\n",
    "    \n",
    "    # calc the cox negative log likelihood \n",
    "    risk_exp = tf.exp(risks)  # exp \n",
    "    \n",
    "    partial_sum = tf.cumsum(risk_exp, axis=1, reverse=True)  # cumsum\n",
    "    log_at_risk = tf.log(partial_sum)\n",
    "    diff = risks - log_at_risk  # sub\n",
    "    \n",
    "    times = tf.multiply(diff, observed, name='times')  # deal with the censored\n",
    "    loss = -tf.reduce_sum(times, axis=1, name='loss')\n",
    "    return risks, risk_exp, partial_sum, log_at_risk, diff, times, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(group_size):\n",
    "    input_img_list = []\n",
    "    for i in range(group_size):\n",
    "        inp = Input(shape=(IMG_SIZE, IMG_SIZE, CHANNEL), name='input_img'+str(i))\n",
    "        input_img_list.append(inp)\n",
    "    \n",
    "    input_censor = Input(shape=(group_size, ), name='input_censor')\n",
    "    \n",
    "    risks = [risk_model(img) for img in input_img_list]\n",
    "    \n",
    "    risk_layer = concatenate(risks, name='risk_concat')\n",
    "      \n",
    "    # build loss layer\n",
    "    loss = Lambda(lambda x: compute_loss(*x))([risk_layer, input_censor])\n",
    "    \n",
    "    model = Model(inputs=[*input_img_list, input_censor], outputs=loss[6])  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(COX_GROUP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.00005, momentum=0.9, decay=0.9)\n",
    "model.compile(optimizer=optimizer, loss=lambda y_true,y_pred: y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined Fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cindex(risk, survival, censored):\n",
    "    total = 0.0\n",
    "    success = 0.0\n",
    "    for i in range(len(survival)):\n",
    "        for j in range(i + 1, len(survival)):\n",
    "            if risk[i] == risk[j]:\n",
    "                continue  \n",
    "            if censored[i] == 0 and censored[j] == 0:\n",
    "                total = total + 1\n",
    "                if survival[i] > survival[j]:\n",
    "                    if risk[j] > risk[i]:\n",
    "                        success = success + 1\n",
    "                elif survival[j] > survival[i]:\n",
    "                    if risk[i] > risk[j]:\n",
    "                        success = success + 1\n",
    "                elif risk[i] == risk[j]:\n",
    "                    success = success + 1\n",
    "            elif censored[i] == 1 and censored[j] == 0:\n",
    "                if survival[i] >= survival[j]:\n",
    "                    total = total + 1\n",
    "                    if risk[j] > risk[i]: \n",
    "                        success = success + 1\n",
    "            elif censored[j] == 1 and censored[i] == 0:\n",
    "                if survival[j] >= survival[i]:\n",
    "                    total = total + 1\n",
    "                    if risk[i] > risk[j]:\n",
    "                        success = success + 1\n",
    "    return success/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level2_risk(t_risks, train_median):\n",
    "    t_risks_level_2 = []\n",
    "    for risk in t_risks:\n",
    "        if risk <= train_median:\n",
    "            t_risks_level_2.append(1)\n",
    "        else:\n",
    "            t_risks_level_2.append(2)\n",
    "\n",
    "    return np.array(t_risks_level_2)\n",
    "\n",
    "def calc_cindex_level_2(t_risks, t_surs, t_cens, train_median):\n",
    "    t_risks_level_2 = get_level2_risk(t_risks, train_median)\n",
    "    return calc_cindex(t_risks_level_2, t_surs, t_cens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(x, method='max', k=0):\n",
    "    x_np = np.array(x)\n",
    "    if method == 'avg':\n",
    "        return np.average(x_np)\n",
    "    elif method == 'med':\n",
    "        return np.median(x_np)\n",
    "    elif method == 'max':\n",
    "        ind = np.argsort(x_np)[::-1]\n",
    "        if k >= len(ind):\n",
    "            k = len(ind)-1\n",
    "        return x_np[ind[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_img(img, h_cnt, w_cnt, tar_size):\n",
    "    imgs = []\n",
    "    for i in range(0, h_cnt):\n",
    "        for j in range(0, w_cnt):\n",
    "            imgs.append(img[i*tar_size:(i+1)*tar_size, j*tar_size:(j+1)*tar_size, :])\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SPLIT_H = 4\n",
    "IMG_SPLIT_W = 4\n",
    "\n",
    "def predict_whole_img(model, img, merge_method=\"avg\"):\n",
    "    imgs = split_img(img, IMG_SPLIT_H, IMG_SPLIT_W, IMG_SIZE)\n",
    "    risks = model.predict(imgs)\n",
    "    return np.average(risks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Predict risk of each pid in test_dir. \n",
    "    Each patient(pid) may have multiple ROI imgs, each img cut into 16 tile img, and their average risk as the risk of this picture.\n",
    "    The risk of patient(pid) is the highest risk of all the pictures of this patient.\n",
    "\"\"\" \n",
    "def predict_dir(model, test_dir, pids):\n",
    "    risks =[]\n",
    "    for pid in pids:\n",
    "        p_risks = []\n",
    "        for filename in glob(test_dir+'/*'+pid+'*'):\n",
    "            img = img_to_array(load_img(filename))/255\n",
    "            risk = predict_whole_img(model, img, merge_method=\"avg\")\n",
    "            p_risks.append(risk)\n",
    "        risks.append(select(p_risks, method=\"max\"))\n",
    "    return risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_test(risks, censored):\n",
    "    risks = np.reshape(risks, -1)\n",
    "    censored = np.reshape(censored, -1)\n",
    "    \n",
    "    observed = 1 - censored\n",
    "\n",
    "    # calc the Cox negative log likelihood \n",
    "    risk_exp = np.exp(risks)  # exp \n",
    "    \n",
    "    risk_exp = risk_exp[::-1]\n",
    "    partial_sum = np.cumsum(risk_exp)\n",
    "    partial_sum = partial_sum[::-1]\n",
    "    \n",
    "    log_at_risk = np.log(partial_sum)\n",
    "    diff = risks - log_at_risk  # sub\n",
    "    \n",
    "    times = np.multiply(diff, observed)  # deal with the censored\n",
    "    loss = -np.sum(times)\n",
    "    \n",
    "    return risks, risk_exp, partial_sum, log_at_risk, diff, times, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the prognosis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    history = model.fit_generator(gen, epochs=1, steps_per_epoch=5)\n",
    "    # predict train\n",
    "    tr_risks = predict_dir(risk_model, train_dir, tr_pids)\n",
    "    tr_loss = compute_loss_test(tr_risks, tr_cens)\n",
    "    tr_cindex_v2 = calc_cindex_level_2(tr_risks, tr_surs, tr_cens, np.median(tr_risks))\n",
    "\n",
    "    print(\"train loss:\", tr_loss[6], \", cindex level2:\", tr_cindex_v2)\n",
    "\n",
    "    # tuning \n",
    "    v_risks = predict_dir(risk_model, tune_dir, v_pids)\n",
    "    v_loss = compute_loss_test(v_risks, v_cens)\n",
    "    v_cindex_v2 = calc_cindex_level_2(v_risks, v_surs, v_cens, np.median(tr_risks))\n",
    "\n",
    "    print(\"tuning loss:\", v_loss[6], \", cindex level2:\", v_cindex_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model.save_weights(model_h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tr_risks = predict_dir(risk_model, train_dir, tr_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_median = np.median(_tr_risks)\n",
    "tr_mean = np.mean(_tr_risks)\n",
    "tr_std = np.std(_tr_risks)\n",
    "print(\"risk_median=%.4f, risk_mean=%.4f, risk_std=%.4f\" % (tr_median))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "189px",
    "width": "212px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "354px",
    "left": "66px",
    "top": "111.7px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 259.79999999999995,
   "position": {
    "height": "281px",
    "left": "1284px",
    "right": "20px",
    "top": "135px",
    "width": "381px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
