{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.applications import DenseNet121, VGG16, VGG19, InceptionV3, ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Activation, concatenate, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDirectoryIterator import MyDirectoryIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for training, tuning and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COX_GROUP_SIZE = 32  # tile group size for compute cox loss\n",
    "MODEL_BATCH_SIZE = 2  # batch size for model training\n",
    "\n",
    "EPOCHS = 100\n",
    "IMG_ORI_SIZE = 1196  # roi img size\n",
    "IMG_SIZE = 224  # tile img size for network input\n",
    "CHANNEL = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant from model training\n",
    "RISK_MEDIAN = -0.6851 \n",
    "RISK_MEAN = -0.6993\n",
    "RISK_STD = 0.3014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"load\"\n",
    "model_h5_file = '../model/prognosis.h5'\n",
    "model_h5_file = os.path.abspath(model_h5_file)\n",
    "test_dir = '../data/prognosis'\n",
    "metadata_path = '../data/metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator and data augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pid_by_filename(filename):\n",
    "    return filename.split('_')[0][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(img_dir):\n",
    "    filenames = glob(img_dir+'/*')\n",
    "    pids = []\n",
    "    for filename in filenames:\n",
    "        pid = get_pid_by_filename(os.path.basename(filename)) \n",
    "        print(pid)\n",
    "        pids.append(pid)\n",
    "    return np.array(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read surival and censor data for pids\n",
    "censor_map = {\"Dead\": 0, \"Alive\": 1}\n",
    "def load_metadata(path, pids):\n",
    "    metadata = pd.read_csv(path)\n",
    "    metadata_dict = dict()\n",
    "    for _, row in metadata.iterrows():\n",
    "        metadata_dict[row['TCGA ID']] = row\n",
    "    \n",
    "    survival = np.empty((len(pids), 1),  dtype='int32')\n",
    "    censored = np.empty((len(pids), 1), dtype='int32')\n",
    "    sur_map = {}\n",
    "    cen_map = {}\n",
    "    \n",
    "    error = False\n",
    "    for i, pid in enumerate(pids):\n",
    "        try:\n",
    "            sur_map[pid] = survival[i] = metadata_dict[pid]['survival']\n",
    "            cen_map[pid] = censored[i] = censor_map[metadata_dict[pid]['censored']]\n",
    "        except KeyError:\n",
    "            error = True\n",
    "            print(\"keyerror for\", pid)\n",
    "    if error:\n",
    "        raise Exception(\"load metadata error\")\n",
    "        \n",
    "    return survival, censored, sur_map, cen_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pids = load_imgs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, _, t_sur_map, t_cen_map = load_metadata(metadata_path, t_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "t_pids = np.unique(t_pids) \n",
    "t_surs = np.array([t_sur_map[pid] for pid in t_pids])\n",
    "t_cens = np.array([t_cen_map[pid] for pid in t_pids])\n",
    "\n",
    "indices = np.argsort(t_surs)\n",
    "\n",
    "t_pids = t_pids[indices]\n",
    "t_surs = t_surs[indices]\n",
    "t_cens = t_cens[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build risk model with VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights=None, include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer = base_model.get_layer('block5_pool')\n",
    "\n",
    "conv_model = Model(inputs=base_model.input,\n",
    "                   outputs=transfer_layer.output)\n",
    "\n",
    "new_model = Sequential()\n",
    "\n",
    "new_model.add(conv_model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dropout(0.3))\n",
    "new_model.add(Dense(1))\n",
    "\n",
    "risk_model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build prognostic model with COX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Need not sort by survival, besause it've been done in data generator, which make the training more stable.\n",
    "def compute_loss(risks, censored):   \n",
    "    observed = 1 - censored\n",
    "    \n",
    "    # calc the cox negative log likelihood \n",
    "    risk_exp = tf.exp(risks)  # exp \n",
    "    \n",
    "    partial_sum = tf.cumsum(risk_exp, axis=1, reverse=True)  # cumsum\n",
    "    log_at_risk = tf.log(partial_sum)\n",
    "    diff = risks - log_at_risk  # sub\n",
    "    \n",
    "    times = tf.multiply(diff, observed, name='times')  # deal with the censored\n",
    "    loss = -tf.reduce_sum(times, axis=1, name='loss')\n",
    "    return risks, risk_exp, partial_sum, log_at_risk, diff, times, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(group_size):\n",
    "    input_img_list = []\n",
    "    for i in range(group_size):\n",
    "        inp = Input(shape=(IMG_SIZE, IMG_SIZE, CHANNEL), name='input_img'+str(i))\n",
    "        input_img_list.append(inp)\n",
    "    \n",
    "    input_censor = Input(shape=(group_size, ), name='input_censor')\n",
    "    \n",
    "    risks = [risk_model(img) for img in input_img_list]\n",
    "    \n",
    "    risk_layer = concatenate(risks, name='risk_concat')\n",
    "      \n",
    "    # build loss layer\n",
    "    loss = Lambda(lambda x: compute_loss(*x))([risk_layer, input_censor])\n",
    "    \n",
    "    model = Model(inputs=[*input_img_list, input_censor], outputs=loss[6])  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(COX_GROUP_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined Fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cindex(risk, survival, censored):\n",
    "    total = 0.0\n",
    "    success = 0.0\n",
    "    for i in range(len(survival)):\n",
    "        for j in range(i + 1, len(survival)):\n",
    "            if risk[i] == risk[j]:\n",
    "                continue  \n",
    "            if censored[i] == 0 and censored[j] == 0:\n",
    "                total = total + 1\n",
    "                if survival[i] > survival[j]:\n",
    "                    if risk[j] > risk[i]:\n",
    "                        success = success + 1\n",
    "                elif survival[j] > survival[i]:\n",
    "                    if risk[i] > risk[j]:\n",
    "                        success = success + 1\n",
    "                elif risk[i] == risk[j]:\n",
    "                    success = success + 1\n",
    "            elif censored[i] == 1 and censored[j] == 0:\n",
    "                if survival[i] >= survival[j]:\n",
    "                    total = total + 1\n",
    "                    if risk[j] > risk[i]: \n",
    "                        success = success + 1\n",
    "            elif censored[j] == 1 and censored[i] == 0:\n",
    "                if survival[j] >= survival[i]:\n",
    "                    total = total + 1\n",
    "                    if risk[i] > risk[j]:\n",
    "                        success = success + 1\n",
    "    return success/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level2_risk(t_risks, train_median):\n",
    "    t_risks_level_2 = []\n",
    "    for risk in t_risks:\n",
    "        if risk <= train_median:\n",
    "            t_risks_level_2.append(1)\n",
    "        else:\n",
    "            t_risks_level_2.append(2)\n",
    "\n",
    "    return np.array(t_risks_level_2)\n",
    "\n",
    "def calc_cindex_level_2(t_risks, t_surs, t_cens, train_median):\n",
    "    t_risks_level_2 = get_level2_risk(t_risks, train_median)\n",
    "    return calc_cindex(t_risks_level_2, t_surs, t_cens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(x, method='max', k=0):\n",
    "    x_np = np.array(x)\n",
    "    if method == 'avg':\n",
    "        return np.average(x_np)\n",
    "    elif method == 'med':\n",
    "        return np.median(x_np)\n",
    "    elif method == 'max':\n",
    "        ind = np.argsort(x_np)[::-1]\n",
    "        if k >= len(ind):\n",
    "            k = len(ind)-1\n",
    "        return x_np[ind[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_img(img, h_cnt, w_cnt, tar_size):\n",
    "    imgs = []\n",
    "    for i in range(0, h_cnt):\n",
    "        for j in range(0, w_cnt):\n",
    "            imgs.append(img[i*tar_size:(i+1)*tar_size, j*tar_size:(j+1)*tar_size, :])\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SPLIT_H = 4\n",
    "IMG_SPLIT_W = 4\n",
    "\n",
    "def predict_whole_img(model, img, merge_method=\"avg\"):\n",
    "    imgs = split_img(img, IMG_SPLIT_H, IMG_SPLIT_W, IMG_SIZE)\n",
    "    risks = model.predict(imgs)\n",
    "    return np.average(risks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Predict risk of each pid in test_dir. \n",
    "    Each patient(pid) may have multiple ROI imgs, each img cut into 16 tile img, and their average risk as the risk of this picture.\n",
    "    The risk of patient(pid) is the highest risk of all the pictures of this patient.\n",
    "\"\"\" \n",
    "def predict_dir(model, test_dir, pids):\n",
    "    risks =[]\n",
    "    for pid in pids:\n",
    "        p_risks = []\n",
    "        for filename in glob(test_dir+'/*'+pid+'*'):\n",
    "            img = img_to_array(load_img(filename))/255\n",
    "            risk = predict_whole_img(model, img, merge_method=\"avg\")\n",
    "            p_risks.append(risk)\n",
    "        risks.append(select(p_risks, method=\"max\"))\n",
    "    return np.array(risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_test(risks, censored):\n",
    "    risks = np.reshape(risks, -1)\n",
    "    censored = np.reshape(censored, -1)\n",
    "    \n",
    "    observed = 1 - censored\n",
    "\n",
    "    # calc the Cox negative log likelihood \n",
    "    risk_exp = np.exp(risks)  # exp \n",
    "    \n",
    "    risk_exp = risk_exp[::-1]\n",
    "    partial_sum = np.cumsum(risk_exp)\n",
    "    partial_sum = partial_sum[::-1]\n",
    "    \n",
    "    log_at_risk = np.log(partial_sum)\n",
    "    diff = risks - log_at_risk  # sub\n",
    "    \n",
    "    times = np.multiply(diff, observed)  # deal with the censored\n",
    "    loss = -np.sum(times)\n",
    "    \n",
    "    return risks, risk_exp, partial_sum, log_at_risk, diff, times, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the prognosis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "risk_model.load_weights(model_h5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# _tr_risks = predict_dir(risk_model, train_dir, tr_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_t_risks = predict_dir(risk_model, test_dir, t_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"risks_level_2 cindex:\", calc_cindex_level_2(_t_risks, t_surs, t_cens, RISK_MEDIAN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(filename, risks, train_median, train_mean, train_std, pids, cens, surs):\n",
    "    risks_level_2 = get_level2_risk(risks, train_median)\n",
    "\n",
    "    # zscore risks\n",
    "    risks = (risks-train_mean) / train_std\n",
    "    \n",
    "    header = (\"pid\", \"censored\", \"survival\", \"risks\", \"risks_level_2\")\n",
    "    data = list(zip(pids, cens, surs, risks, risks_level_2))\n",
    "\n",
    "    print(header)\n",
    "    with open(filename, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(header)\n",
    "        for row in data:\n",
    "            print(row)\n",
    "            csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results('../result/result_progosis.csv', _t_risks, RISK_MEDIAN, RISK_MEAN, RISK_STD, t_pids, t_cens, t_surs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "189px",
    "width": "212px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "354px",
    "left": "66px",
    "top": "111.7px",
    "width": "305px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 259.79999999999995,
   "position": {
    "height": "281px",
    "left": "1284px",
    "right": "20px",
    "top": "135px",
    "width": "381px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
